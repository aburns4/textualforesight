# -*- coding: utf-8 -*-
"""process_aitw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SIE7DCq8EMvO5csxR5XycxGbc16744Vu

Copyright 2023 The Google Research Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import sys
sys.path.append('./google-research')

from android_in_the_wild import visualization_utils, action_matching
import argparse
import tensorflow as tf
import random
import json
import glob
import numpy as np
import time
import os
import random
from collections import defaultdict, Counter
from typing import List
from PIL import Image
dataset_directories = {
    'general': 'gs://gresearch/android-in-the-wild/general/*',
    'google_apps': 'gs://gresearch/android-in-the-wild/google_apps/*',
    'install': 'gs://gresearch/android-in-the-wild/install/*',
    'single': 'gs://gresearch/android-in-the-wild/single/*',
    'web_shopping': 'gs://gresearch/android-in-the-wild/web_shopping/*',
}

parser = argparse.ArgumentParser('Create pretrain annotations for Fortune objective')
parser.add_argument('--dataset',
                    type=str,
                    default="aitw",
                    help='specify which dataset to process')
parser.add_argument('--folder',
                    type=str,
                    default="st_at_st1",
                    help='specify output_folder')
parser.add_argument('--include_icons',
                    type=bool,
                    default=False,
                    help='specify whether icon text should be included')
parser.add_argument('--dataset_subset',
                    type=str,
                    choices=["general", "google_apps", "install", "web_shopping"],
                    help='specify which dataset subset to process')
parser.add_argument('--start_range',
                    type=int,
                    default=0,
                    help='specify start of range to process')
parser.add_argument('--end_range',
                    type=int,
                    default=5000,
                    help='specify end of range to process')

class UI:
  def __init__(self):
    self.leaves = []        
    self.texts = []
  def recurse(self, node):
    if node:
      if 'children' in node and len(node['children']) > 0:
        for child in node['children']:
          self.recurse(child)
      else:
        # if not node["has-background"]:
        self.leaves = self.leaves + [node["bounds"]]
        if "text" in node:
          self.texts = self.texts + [node["text"]]

def get_motif_files():
    pretrain_traces = glob.glob('motif/*/*/*')
    set_traces = [(fp, '/'.join(fp.split('/')[-2:])) for fp in pretrain_traces]
    dict_traces = defaultdict(list)
    for entry in set_traces:
        dict_traces[entry[1]].append(entry[0])
    for_pretrain = []
    for x in dict_traces.values():
      try:
        with open(x[0] + '/metadata.json') as f:
          seq = json.load(f)["views"]
        # print(seq)
        seq = [x[0] + '/view_hierarchies/' + v for v in seq]
        for_pretrain.append(seq)
      except:
        print("missing metadata")
        continue
    return for_pretrain

def normalize_bbox(bbox, image):
    w = image.width
    h = image.height
    return [bbox[0] / w, bbox[1] / h, bbox[2] / w, bbox[3] / h]

def convert_view_to_screen_dims(bbox, scale_x, scale_y, adjustment=0):
    # need to convert to screen localization
    bbox_width = bbox[2] - bbox[0]
    bbox_height = bbox[3] - bbox[1]
    new_x1 = bbox[0] * scale_x
    new_y1 = (bbox[1] * scale_y) - adjustment
    new_x2 = (bbox[0] + bbox_width) * scale_x
    new_y2 = ((bbox[1] + bbox_height) * scale_y) - adjustment
    if new_x1 < 0:
      new_x1 = 0.0
    if new_y1 < 0:
      new_y1 = 0.0
    if new_x2 < 0:
      new_x2 = 0.0
    if new_y2 < 0:
      new_y2 = 0.0

    assert new_x1 >= 0.0 and new_y1 >= 0.0 and new_x2 >= 0.0 and new_y2 >= 0.0
    return [new_x1, new_y1, new_x2, new_y2]

def get_bbox_scale(image_path,
                   root_path='/projectnb2/ivc-ml/aburns4/combined',
                   vh_w = 1440, vh_h = 2560, adjustment = 0):
    full_im_path = (os.path.join(root_path, image_path) if image_path not in root_path else root_path)
    image = Image.open(full_im_path).convert('RGB')

    im_w = image.width
    im_h = image.height
    scale_x = im_w / vh_w
    scale_y = (im_h + adjustment) / vh_h
    return scale_x, scale_y, image

def get_motif_vh_dims(vh_bounds, fp, exceptions="widget_exception_dims.json"):
    trace = fp.split('/')[-3]
    with open(exceptions) as f:
        widget_exceptions = json.load(f)

    if trace in widget_exceptions:
        vh_dims = widget_exceptions[trace]
        vh_dims = [int(x) for x in vh_dims]
        return vh_dims
    elif vh_bounds[0] == vh_bounds[1] == 0:
        return vh_bounds[2:]
    else:
        return None

# Returns true if two rectangles(l1, r1)
# and (l2, r2) overlap
def do_overlap(rect1, rect2):
  left1, top1, right1, bottom1 = rect1
  left2, top2, right2, bottom2 = rect2

  # If one rectangle is on left side of other
  if left1 > right2 or left2 > right1:
      return False

  # If one rectangle is above other
  if bottom1 > top2 or bottom2 > top1:
      return False
  return True

def not_single_color(image, norm_bbox, w, h):
  img_bbox = [norm_bbox[0]*w, norm_bbox[1]*h, norm_bbox[2]*w, norm_bbox[3]*h]
  ui_crop = image.copy().crop(img_bbox)
  extrema = ui_crop.convert("L").getextrema()
  if extrema is None:
      return False
  
  min_px, max_px = extrema
  one_value = (min_px == max_px)
  if one_value:
      return False
  
  return True

def get_match_from_unknown(screen_norm_bboxes, norm_action, perimeter_bbox, img, screen_w, screen_h):
  # assert bboxes have non-zero area
  overlaps = []
  for bbox in screen_norm_bboxes:
    if (bbox[2]-bbox[0]) == 0 or (bbox[3]-bbox[1]) == 0:
      overlaps.append(False)
    else:
      overlaps.append(do_overlap(bbox, perimeter_bbox))
  if overlaps.count(True) > 0:
    # get closest by midpoint
    dists_idxs = []
    for i in range(len(screen_norm_bboxes)):
      if overlaps[i]:
        midpoint = [(screen_norm_bboxes[i][2]-screen_norm_bboxes[i][0])/2,
                    (screen_norm_bboxes[i][3]-screen_norm_bboxes[i][1])/2]
        dists_idxs.append([np.linalg.norm(np.array(norm_action)-np.array(midpoint)), i])
    
    dists = [x[0] for x in dists_idxs]
    assert dists.count(min(dists)) == 1
    dists = sorted(dists, key = lambda x: x[0])
    # confirm it does not fall into back bar and is not single color
    min_dist_bbox_idx = dists[0][1]
    back_bar_boundary = (screen_h-100)/screen_h
    selected = screen_norm_bboxes[min_dist_bbox_idx]
    if selected[3] <= back_bar_boundary and not_single_color(img, selected, screen_w, screen_h):
      return selected

  # confirm not all one color
  if not_single_color(img, perimeter_bbox, screen_w, screen_h):
    return perimeter_bbox
  return None

def get_match_of_many(matched_bboxes, action):
  # both are in normalized screen coords
  areas = [(bb[2] - bb[0]) * (bb[3] - bb[1]) for bb in matched_bboxes]
  bbox_mdpts = [np.array([(x[0] + x[2])/2, (x[1] + x[3])/2]) for x in matched_bboxes]
  action = np.array(action)
  dists = [np.linalg.norm(x-action) for x in bbox_mdpts]
  dist_idx = dists.index(min(dists))
  if dists.count(min(dists)) > 1:
    kept_bbox = None
    kept_bbox_area = 1000000000
    for i in range(len(matched_bboxes)):
      if dists[i] == min(dists):
        if kept_bbox is None:
          kept_bbox = matched_bboxes[i]
          kept_bbox_area = areas[i]
        else:
          if areas[i] < kept_bbox_area:
            kept_bbox = matched_bboxes[i]
            kept_bbox_area = areas[i]
    return kept_bbox
  else:
    assert dists.count(min(dists)) == 1
    return matched_bboxes[dist_idx]

def match_action(action, bboxes):
  ax, ay = action
  action_in_bbox = []
  for box in bboxes:
    x1, y1, x2, y2 = box
    if x1 <= ax <= x2 and y1 <= ay <= y2:
      action_in_bbox.append(True)
    else:
      action_in_bbox.append(False)
  return action_in_bbox

def motif_sa(trace_paths, errors):
  trace_root = ['/'.join(t.split('/')[1:-2]) for t in trace_paths]
  trace_root = list(set(trace_root))
  assert len(trace_root) == 1
  
  trace_root = trace_root[0]
  print(trace_root)
  
  samples = []
  for i in range(len(trace_paths) - 1):
    ui = UI()
    vh_path = trace_paths[i]
    screen_path = vh_path.replace("view_hierarchies", "screens")
    st = vh_path.split('/')[-1]
    
    with open(os.path.join("motif", trace_root, "metadata.json")) as f:
      data = json.load(f)
      at = data["gestures"][st]
      if len(at) == 0:
        errors["not_tap"] += 1
        continue
      elif len(at) > 1:
        if not visualization_utils.is_tap_action(at[0], at[-1]):
          errors["not_tap"] += 1
          continue
      at = at[0]
    try:
      with open(vh_path) as f:
        vh = json.load(f)
    except:
      errors["vh_errors"] += 1
      continue

    try:
      vh_w, vh_h = get_motif_vh_dims(vh["activity"]["root"]["bounds"], vh_path)
      if vh_w == 0 or vh_h == 0:
        errors["vh_size_errors"] += 1
        continue
    except:
      errors["vh_size_errors"] += 1
      continue
    
    sx, sy, loaded_image = get_bbox_scale(screen_path,
                                          root_path="/projectnb2/ivc-ml/aburns4/LAVIS/pretrain_stuff",
                                          vh_w=vh_w,
                                          vh_h=vh_h,
                                          adjustment=65)
    w, h = loaded_image.size
    go_back_banner_bbox = [0, (h-100)/h, 1, 1]
    keyboard_bbox = [0, 5.075*(h-100)/(h*8), 1, (h-100)/h]
    
    ui.recurse(vh["activity"]["root"])
    all_bboxes = ui.leaves
    screen_bboxes = []
    for vh_bb in all_bboxes:
      new_bb = convert_view_to_screen_dims(vh_bb, sx, sy, adjustment=65)
      if new_bb[2] - new_bb[0] <= 10 or new_bb[3] - new_bb[1] <= 10:
        continue
      if new_bb[2] - new_bb[0] == w or new_bb[3] - new_bb[1] == h:
        continue
      if new_bb not in screen_bboxes:
        screen_bboxes.append(new_bb)
    screen_norm_bboxes = [normalize_bbox(screen_bb, loaded_image) for screen_bb in screen_bboxes]


    action_bools = match_action(at, screen_norm_bboxes)
    num_matched = action_bools.count(True)

    matched_bboxes = []
    for idx in range(len(action_bools)):
      if action_bools[idx]:
        matched_bboxes.append(screen_norm_bboxes[idx])

    st1 = trace_paths[i+1].split('/')[-1]
    if num_matched == 1:
      matched = action_bools.index(True)
      bbox = screen_norm_bboxes[matched]

      str_bbox = '%.2f %.2f %.2f %.2f' % (bbox[0], bbox[1], bbox[2], bbox[3])
      samples.append((trace_root, st, st1, str_bbox))
    else:
      if num_matched == 0:
        errors["no_bbox"] += 1
        if go_back_banner_bbox[1] <= at[1] <= 1:
          errors["go_back_banner"] += 1
          continue
        elif len(screen_bboxes) == 0:
          errors["no_bbox_at_all"] += 1
          around_action = [max(0, at[0] - (65/w)),
                           max(0, at[1] - (65/h)),
                           min(1, at[0] + (65/w)),
                           min(1, at[1] + (65/h))]
          str_bbox = '%.2f %.2f %.2f %.2f' % (around_action[0], around_action[1], around_action[2], around_action[3])
          samples.append((trace_root, st, st1, str_bbox))
        elif 1 <= len(screen_norm_bboxes) <= 4:
          errors["no_bbox_few_bbox"] += 1
          around_action = [max(0, at[0] - (65/w)),
                           max(0, at[1] - (65/h)),
                           min(1, at[0] + (65/w)),
                           min(1, at[1] + (65/h))]
          str_bbox = '%.2f %.2f %.2f %.2f' % (around_action[0], around_action[1], around_action[2], around_action[3])
          samples.append((trace_root, st, st1, str_bbox))
        elif keyboard_bbox[1] <= at[1] <= keyboard_bbox[3] and all([x[3] < keyboard_bbox[1] for x in screen_norm_bboxes]):
          errors["keyboard_region"] += 1
        elif "PQRS" in ui.texts:
          errors["likey_android_dialer"] += 1
        else:
          errors["no_bbox_unknown"] += 1
          perimeter_bbox = [max(0, at[0] - (65/w)),
                            max(0, at[1] - (65/h)),
                            min(1, at[0] + (65/w)),
                            min(1, at[1] + (65/h))]
          kept_bbox = get_match_from_unknown(screen_norm_bboxes, at, perimeter_bbox, loaded_image, w, h)
          if kept_bbox is not None:
            str_bbox = '%.2f %.2f %.2f %.2f' % (kept_bbox[0], kept_bbox[1], kept_bbox[2], kept_bbox[3])
            samples.append((trace_root, st, st1, str_bbox))
          else:
              errors["no_bbox_unknown_removed"] += 1
      else:
        kept_bbox = get_match_of_many(matched_bboxes, at)
        str_bbox = '%.2f %.2f %.2f %.2f' % (kept_bbox[0], kept_bbox[1], kept_bbox[2], kept_bbox[3])
        samples.append((trace_root, st, st1, str_bbox))
        errors["many_bbox"] += 1
  return samples, errors 

def match_ui_locs(ui_locations, yx_touch):
  original_annotation_positions = ui_locations
  og_match_bools = list(action_matching._yx_in_bounding_boxes(yx_touch, original_annotation_positions))
  if og_match_bools.count(True) == 1:
    return og_match_bools, 0
  elif og_match_bools.count(True) == 0:
    in_btw_annotation_positions = action_matching._resize_annotation_bounding_boxes(
      ui_locations,
      1.2,
      1.2,
    )
    in_btw_match_bools = list(action_matching._yx_in_bounding_boxes(yx_touch, in_btw_annotation_positions))
    if in_btw_match_bools.count(True) == 1:
      return in_btw_match_bools, 1
    elif in_btw_match_bools.count(True) > 1:
      return in_btw_match_bools, -1
    else:
      # == 0
      in_btw_annotation_positions = action_matching._resize_annotation_bounding_boxes(
      ui_locations,
      action_matching.ANNOTATION_WIDTH_AUGMENT_FRACTION,
      action_matching.ANNOTATION_HEIGHT_AUGMENT_FRACTION,
    )
    in_btw_match_bools = list(action_matching._yx_in_bounding_boxes(yx_touch, in_btw_annotation_positions))
    if in_btw_match_bools.count(True) == 1:
      return in_btw_match_bools, 2
    else:
      # 0 or > 1
      return in_btw_match_bools, -1
  else:
    # for some reason > 1
    return og_match_bools, -1

def correct_action_matching(ui_locations, yx_touch, num_matching):
  touch_y, touch_x = yx_touch
  touch_coords = np.array((touch_x, touch_y))
  if num_matching == 0:
    widths = [x[3] for x in ui_locations]
    avg_width = sum(widths) / len(widths)
    heights = [x[2] for x in ui_locations]
    avg_height = sum(heights) / len(heights)

    start_y = touch_y - 0.5*avg_height
    start_x = touch_x - 0.5*avg_width
    return [start_y, start_x, avg_height, avg_width]
  else:
    assert num_matching > 1
    midpts = []
    for (y, x, h, w) in ui_locations:
      m_x = x + 0.5*w
      m_y = y + 0.5*h
      midpts.append(np.array([m_x, m_y]))
    dists = [np.linalg.norm(touch_coords-bbox_mid) for bbox_mid in midpts]
    index_min = np.argmin(dists)
    return ui_locations[index_min]

def aitw_sa(ex, reasons_removed, matching_level):
  app_id = ex.features.feature['current_activity'].bytes_list.value[0].decode('utf-8')
  episode_id = ex.features.feature['episode_id'].bytes_list.value[0].decode('utf-8')
  step_id = ex.features.feature['step_id'].int64_list.value[0]
  
  yx_touch = ex.features.feature["results/yx_touch"].float_list.value
  yx_lift = ex.features.feature["results/yx_lift"].float_list.value
  ui_locations = ex.features.feature['image/ui_annotations_positions'].float_list.value
  ui_locations = np.array([ui_locations[i:i+4] for i in range(0, len(ui_locations), 4)]) # y, x, h, w

  if len(ui_locations) == 0:
    reasons_removed["missing_ui_locations"] += 1
    return None, reasons_removed, matching_level

  if yx_touch != [-1.0, -1.0] and visualization_utils.is_tap_action(yx_touch, yx_lift):
    match_bools, level = match_ui_locs(ui_locations, yx_touch)
    matching_level[level] += 1
    num_match_bboxes = list(match_bools).count(True)
    if num_match_bboxes != 1:
      # can't find singular matching element
      if num_match_bboxes > 1:
        reasons_removed["more_than_one_bbox_matchs"] +=1
      else:
        reasons_removed["zero_bbox_matchs"] +=1
        if step_id == 0:
          reasons_removed["zero_0_step_bbox_matchs"] +=1

      # get min dist bbox if many,
      # else make new bbox that has average width and height around action midpoint
      matched_bbox = correct_action_matching(ui_locations, yx_touch, num_match_bboxes)
      matched_bbox = [matched_bbox[1],
                      matched_bbox[0],
                      matched_bbox[1] + matched_bbox[3],
                      matched_bbox[0] + matched_bbox[2]]
      str_bbox = '%.2f %.2f %.2f %.2f' % (matched_bbox[0], matched_bbox[1], matched_bbox[2], matched_bbox[3])
      return (app_id, episode_id, str(step_id), str(step_id + 1), str_bbox), reasons_removed, matching_level
    else:
      assert list(match_bools).count(True) == 1
      action_ui = list(match_bools).index(True)
      matched_bbox = ui_locations[action_ui]

      matched_bbox = [matched_bbox[1],
                      matched_bbox[0],
                      matched_bbox[1] + matched_bbox[3],
                      matched_bbox[0] + matched_bbox[2]]
      str_bbox = '%.2f %.2f %.2f %.2f' % (matched_bbox[0], matched_bbox[1], matched_bbox[2], matched_bbox[3])
      return (app_id, episode_id, str(step_id), str(step_id + 1), str_bbox), reasons_removed, matching_level
  else:
    # not a valid interaction
    reasons_removed["not_tap_action"] += 1

  return None, reasons_removed, matching_level

def longitudinal_sa(root_path="/projectnb2/ivc-ml/aburns4/LAVIS/pretrain_stuff/longitudinal"):
  all_files = glob.glob(os.path.join(root_path, "*/graph.json"))
  print(len(all_files))
  triplets = []
  actions = []
  screen_w = 1080
  screen_h = 2220
  no_next_state = 0
  for fp in all_files:
    app = fp.split('/')[-2]
    print(app)
    with open(fp) as f:
      graph = json.load(f)
    for state in graph:
      for elem in graph[state]:
        next_state = elem["result_uuid"]
        if next_state is None:
          no_next_state += 1
          continue
       
        bounds = elem['bounds'][1:-1].replace("][",",").split(",")
        bounds = [float(x) for x in bounds]
        norm_bounds = [bounds[0] / screen_w,
                       bounds[1] / screen_h,
                       bounds[2] / screen_w,
                       bounds[3] / screen_h]
        norm_bounds_str = '%.2f %.2f %.2f %.2f' % (norm_bounds[0], norm_bounds[1], norm_bounds[2], norm_bounds[3])

        assert state is not None
        assert next_state is not None
        assert norm_bounds_str is not None
        
        triplets.append((app, state, next_state, norm_bounds_str))
        actions.append(elem["action_type"])
  print(set(actions))
  print(len(triplets))
  print(no_next_state)
  return triplets

def main():
  start_time = time.time()

  global args
  args = parser.parse_args()

  num_none = 0
  if args.dataset == "longitudinal":
    triplets = longitudinal_sa()
  elif args.dataset == "aitw":
    dataset_name = args.dataset_subset
    filenames = tf.io.gfile.glob(dataset_directories[dataset_name])
    assert args.start_range >= 0 and args.start_range <= args.end_range
  
    filenames = filenames[args.start_range : args.end_range]
    print(args.start_range, args.end_range, len(filenames))
    raw_dataset = tf.data.TFRecordDataset(filenames, compression_type='GZIP').as_numpy_iterator()

    triplets = []
    i = 0
    action_types = defaultdict(int)
    reasons_removed = defaultdict(int)
    action_match_level = defaultdict(int)
    for d in raw_dataset:
      if i % 1000 == 0:
        print(i)
      
      ex = tf.train.Example()
      ex.ParseFromString(d)

      trip, reasons_removed, action_match_level = aitw_sa(ex, reasons_removed, action_match_level)
      if trip is not None:
        action_types[ex.features.feature['results/action_type'].int64_list.value[0]] += 1
        triplets.append(trip)
      else:
        num_none += 1
      i+=1
    print('%d samples processed for AITW %s, only %d remained valid' % (i, args.dataset_subset, len(triplets)))
    print("\nReasons removed:")
    for reason, count in reasons_removed.items():
      print(reason, count)
    print("\nAction matching level:")
    for level, count in action_match_level.items():
      print(level, count)
    print(action_types)
  elif args.dataset == "motif":
    traces = get_motif_files()
    print(len(traces))

    triplets = []
    errors = defaultdict(int)
    potential_total = 0
    for tr in traces:
      if len(tr) == 0:
        continue
      potential_total += (len(tr) - 1)
      tr_samples, errors = motif_sa(tr, errors) 
      triplets += tr_samples
    print("Total kept samples %d of %d potential samples" % (len(triplets), potential_total))
    for k,v in errors.items():
      print(k,v)
  else:
    raise ValueError("Dataset is not supported.")

  save_folder = os.path.join("spotlight_jsons", args.dataset, args.folder)
  save_path = os.path.join(save_folder, "triplets.txt")
  if not os.path.exists(save_folder):
    os.makedirs(save_folder)
  
  print(save_path)
  with open(save_path, "a") as f:
    to_write = '\n'.join([','.join(sample) for sample in triplets])
    if os.path.exists(save_path):
      f.write("\n" + to_write)
    else:
      f.write(to_write)

  end_time = time.time()
  seconds = end_time - start_time
  minutes = seconds / 60
  hours = minutes / 60
  print('Time to process %s samples: %.2f seconds / %.2f minutes / %.2f / hours' % (args.dataset, seconds, minutes, hours))

if __name__ == "__main__":
  main()